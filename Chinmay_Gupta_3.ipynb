{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHShjUKJ4B1q"
   },
   "source": [
    "#Review Analysis using RNN/LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqo5UPCo38Wr"
   },
   "source": [
    "###Mounting Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3P-ghK54R3L",
    "outputId": "2b73b60c-2ac8-498f-8df5-9d883e2347e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files, drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1flk1LE4Kv3"
   },
   "source": [
    "###Importing Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s5TVqciV3swj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms,utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tensorflow as tf\n",
    "from __future__ import unicode_literals, print_function, division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df2i9gJR4cCP"
   },
   "source": [
    "###Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S5Zd4A904I8t"
   },
   "outputs": [],
   "source": [
    "# loading 500 files from each folder.\n",
    "\n",
    "neg_reviews = glob.glob(\"/content/drive/My Drive/reviews/neg/*.txt\")[:500]\n",
    "pos_reviews = glob.glob(\"/content/drive/My Drive/reviews/pos/*.txt\")[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sFdt9-_DEOg3"
   },
   "outputs": [],
   "source": [
    "max_features = 200\n",
    "\n",
    "#Creating corpus and assigning labels\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "for i in pos_reviews:\n",
    "  with open(i) as fh:\n",
    "    corpus.append(fh.read().replace('\\n',' '))\n",
    "    labels.append([1,0])\n",
    "\n",
    "for i in neg_reviews:\n",
    "  with open(i) as fh:\n",
    "    corpus.append(fh.read().replace('\\n',' '))\n",
    "    labels.append([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jb3htNs9FxzS",
    "outputId": "405f1e9a-a3a5-42ca-c51c-7528610940b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 200) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## using `TfidfVectorizer` from sklearn to generate tf-idf values for every word in each document.\n",
    "vectorizer = TfidfVectorizer(max_features=200, stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKtI6g4GTE3F",
    "outputId": "1f4b2973-98e8-4b84-e4c3-aa63b8c4c228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 311, 200) (300, 311, 200) (700, 2) (300, 2)\n"
     ]
    }
   ],
   "source": [
    "seq_length = -1\n",
    "\n",
    "word_tokenizer = vectorizer.build_tokenizer()\n",
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "#max_features = 200\n",
    "\n",
    "doc_terms_list_train = [word_tokenizer(doc_str) for doc_str in corpus]\n",
    "docs = []\n",
    "for i in range(len(doc_terms_list_train)):\n",
    "  terms = []\n",
    "  for j in range(len(doc_terms_list_train[i])):\n",
    "    w = doc_terms_list_train[i][j]\n",
    "    if w in vocab:\n",
    "      terms.append(w)\n",
    "  if len(terms) > seq_length:\n",
    "    seq_length=len(terms)\n",
    "  docs.append(terms)\n",
    "\n",
    "datasets = np.zeros((X.shape[0],seq_length,max_features))\n",
    "\n",
    "for i in range(len(docs)):\n",
    "  n_padding = seq_length - len(docs[i])\n",
    "\n",
    "  for j in range(len(docs[i])):\n",
    "    w = docs[i][j]\n",
    "    idx = vocab[w]\n",
    "    tfidf_val = X[i,idx]\n",
    "    datasets[i,j+n_padding,idx] = tfidf_val\n",
    "\n",
    "datasets = datasets.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(datasets, y, test_size=0.3, random_state = 1012)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE6QVReA4fwr"
   },
   "source": [
    "###Creating Dataloader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LTi8ya-eFl05"
   },
   "outputs": [],
   "source": [
    "#create train/val dataloader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(X_train),torch.from_numpy(y_train))\n",
    "val_data = TensorDataset(torch.from_numpy(X_val),torch.from_numpy(y_val))\n",
    "\n",
    "train_loader = DataLoader(train_data,shuffle=True,batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data,shuffle=True,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgPyQdGw4jH9"
   },
   "source": [
    "###Creating RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiFm1EE30Fbm",
    "outputId": "b095ea91-629f-4437-d553-994bbc309a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(200, 256, num_layers=2, batch_first=True)\n",
      "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "  def __init__(self,input_size,output_size,hidden_size,n_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.rnn = nn.RNN(input_size,hidden_size,n_layers,batch_first=True)\n",
    "    self.fc1 = nn.Linear(hidden_size,output_size)\n",
    "    self.fc2 = nn.Linear(output_size,2)\n",
    "\n",
    "  def forward(self,x,hidden):\n",
    "    batch_size = x.size()[0]\n",
    "\n",
    "    hidden = self.init_hidden(batch_size)\n",
    "    #print(hidden.size())\n",
    "    rnn_out,hidden = self.rnn(x,hidden)\n",
    "\n",
    "    rnn_out = self.fc1(rnn_out)\n",
    "\n",
    "    last_out = rnn_out[:,-1,:].view(batch_size,-1)\n",
    "\n",
    "    out = F.softmax(self.fc2(last_out))\n",
    "\n",
    "    return out,hidden\n",
    "\n",
    "  def init_hidden(self,batch_size):\n",
    "    hidden = torch.zeros(self.n_layers,batch_size,self.hidden_size).cuda()\n",
    "    return hidden\n",
    "\n",
    "model = Model(200,32,256,2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVsw2fQT4rCr"
   },
   "source": [
    "###Please Activate GPU before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "x1OLJCzordM2"
   },
   "outputs": [],
   "source": [
    "#Using GPU\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "  model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpPH-2mn22Kj",
    "outputId": "cc027e3d-bd59-4867-84fd-c290bb419fa4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/10 Batch:1 Train Loss:0.74687 Val Loss:0.73718\n",
      "Epoch:1/10 Batch:2 Train Loss:0.73665 Val Loss:0.72694\n",
      "Epoch:1/10 Batch:3 Train Loss:0.72692 Val Loss:0.71632\n",
      "Epoch:1/10 Batch:4 Train Loss:0.71615 Val Loss:0.70519\n",
      "Epoch:1/10 Batch:5 Train Loss:0.70481 Val Loss:0.69342\n",
      "Epoch:1/10 Batch:6 Train Loss:0.69344 Val Loss:0.68086\n",
      "Epoch:1/10 Batch:7 Train Loss:0.68062 Val Loss:0.66742\n",
      "Epoch:1/10 Batch:8 Train Loss:0.66720 Val Loss:0.65294\n",
      "Epoch:1/10 Batch:9 Train Loss:0.65303 Val Loss:0.63726\n",
      "Epoch:1/10 Batch:10 Train Loss:0.63729 Val Loss:0.62019\n",
      "Epoch:1/10 Batch:11 Train Loss:0.62016 Val Loss:0.60158\n",
      "Epoch:1/10 Batch:12 Train Loss:0.60151 Val Loss:0.58126\n",
      "Epoch:1/10 Batch:13 Train Loss:0.58103 Val Loss:0.55914\n",
      "Epoch:1/10 Batch:14 Train Loss:0.55898 Val Loss:0.53531\n",
      "Epoch:1/10 Batch:15 Train Loss:0.53575 Val Loss:0.50998\n",
      "Epoch:1/10 Batch:16 Train Loss:0.50976 Val Loss:0.48367\n",
      "Epoch:1/10 Batch:17 Train Loss:0.48330 Val Loss:0.45716\n",
      "Epoch:1/10 Batch:18 Train Loss:0.45714 Val Loss:0.43151\n",
      "Epoch:1/10 Batch:19 Train Loss:0.43133 Val Loss:0.40780\n",
      "Epoch:1/10 Batch:20 Train Loss:0.40774 Val Loss:0.38695\n",
      "Epoch:1/10 Batch:21 Train Loss:0.38711 Val Loss:0.36951\n",
      "Epoch:1/10 Batch:22 Train Loss:0.36957 Val Loss:0.35556\n",
      "Epoch:1/10 Batch:23 Train Loss:0.35561 Val Loss:0.34481\n",
      "Epoch:1/10 Batch:24 Train Loss:0.34490 Val Loss:0.33678\n",
      "Epoch:1/10 Batch:25 Train Loss:0.33682 Val Loss:0.33091\n",
      "Epoch:1/10 Batch:26 Train Loss:0.33092 Val Loss:0.32666\n",
      "Epoch:1/10 Batch:27 Train Loss:0.32668 Val Loss:0.32361\n",
      "Epoch:1/10 Batch:28 Train Loss:0.32363 Val Loss:0.32141\n",
      "Epoch:1/10 Batch:29 Train Loss:0.32141 Val Loss:0.31981\n",
      "Epoch:1/10 Batch:30 Train Loss:0.31981 Val Loss:0.31863\n",
      "Epoch:1/10 Batch:31 Train Loss:0.31863 Val Loss:0.31774\n",
      "Epoch:1/10 Batch:32 Train Loss:0.31774 Val Loss:0.31706\n",
      "Epoch:1/10 Batch:33 Train Loss:0.31707 Val Loss:0.31654\n",
      "Epoch:1/10 Batch:34 Train Loss:0.31654 Val Loss:0.31613\n",
      "Epoch:1/10 Batch:35 Train Loss:0.31613 Val Loss:0.31579\n",
      "Epoch:1/10 Batch:36 Train Loss:0.31579 Val Loss:0.31553\n",
      "Epoch:1/10 Batch:37 Train Loss:0.31553 Val Loss:0.31531\n",
      "Epoch:1/10 Batch:38 Train Loss:0.31531 Val Loss:0.31513\n",
      "Epoch:1/10 Batch:39 Train Loss:0.31513 Val Loss:0.31497\n",
      "Epoch:1/10 Batch:40 Train Loss:0.31497 Val Loss:0.31484\n",
      "Epoch:1/10 Batch:41 Train Loss:0.31485 Val Loss:0.31473\n",
      "Epoch:1/10 Batch:42 Train Loss:0.31474 Val Loss:0.31464\n",
      "Epoch:1/10 Batch:43 Train Loss:0.31464 Val Loss:0.31456\n",
      "Epoch:1/10 Batch:44 Train Loss:0.31456 Val Loss:0.31449\n",
      "Epoch:2/10 Batch:45 Train Loss:0.31449 Val Loss:0.31443\n",
      "Epoch:2/10 Batch:46 Train Loss:0.31443 Val Loss:0.31437\n",
      "Epoch:2/10 Batch:47 Train Loss:0.31437 Val Loss:0.31432\n",
      "Epoch:2/10 Batch:48 Train Loss:0.31432 Val Loss:0.31428\n",
      "Epoch:2/10 Batch:49 Train Loss:0.31428 Val Loss:0.31424\n",
      "Epoch:2/10 Batch:50 Train Loss:0.31424 Val Loss:0.31421\n",
      "Epoch:2/10 Batch:51 Train Loss:0.31421 Val Loss:0.31417\n",
      "Epoch:2/10 Batch:52 Train Loss:0.31417 Val Loss:0.31415\n",
      "Epoch:2/10 Batch:53 Train Loss:0.31415 Val Loss:0.31412\n",
      "Epoch:2/10 Batch:54 Train Loss:0.31412 Val Loss:0.31410\n",
      "Epoch:2/10 Batch:55 Train Loss:0.31410 Val Loss:0.31408\n",
      "Epoch:2/10 Batch:56 Train Loss:0.31408 Val Loss:0.31406\n",
      "Epoch:2/10 Batch:57 Train Loss:0.31406 Val Loss:0.31404\n",
      "Epoch:2/10 Batch:58 Train Loss:0.31404 Val Loss:0.31402\n",
      "Epoch:2/10 Batch:59 Train Loss:0.31402 Val Loss:0.31401\n",
      "Epoch:2/10 Batch:60 Train Loss:0.31401 Val Loss:0.31399\n",
      "Epoch:2/10 Batch:61 Train Loss:0.31399 Val Loss:0.31398\n",
      "Epoch:2/10 Batch:62 Train Loss:0.31398 Val Loss:0.31397\n",
      "Epoch:2/10 Batch:63 Train Loss:0.31397 Val Loss:0.31395\n",
      "Epoch:2/10 Batch:64 Train Loss:0.31395 Val Loss:0.31394\n",
      "Epoch:2/10 Batch:65 Train Loss:0.31394 Val Loss:0.31393\n",
      "Epoch:2/10 Batch:66 Train Loss:0.31393 Val Loss:0.31392\n",
      "Epoch:2/10 Batch:67 Train Loss:0.31392 Val Loss:0.31391\n",
      "Epoch:2/10 Batch:68 Train Loss:0.31391 Val Loss:0.31390\n",
      "Epoch:2/10 Batch:69 Train Loss:0.31390 Val Loss:0.31390\n",
      "Epoch:2/10 Batch:70 Train Loss:0.31390 Val Loss:0.31389\n",
      "Epoch:2/10 Batch:71 Train Loss:0.31389 Val Loss:0.31388\n",
      "Epoch:2/10 Batch:72 Train Loss:0.31388 Val Loss:0.31387\n",
      "Epoch:2/10 Batch:73 Train Loss:0.31387 Val Loss:0.31387\n",
      "Epoch:2/10 Batch:74 Train Loss:0.31387 Val Loss:0.31386\n",
      "Epoch:2/10 Batch:75 Train Loss:0.31386 Val Loss:0.31385\n",
      "Epoch:2/10 Batch:76 Train Loss:0.31385 Val Loss:0.31385\n",
      "Epoch:2/10 Batch:77 Train Loss:0.31385 Val Loss:0.31384\n",
      "Epoch:2/10 Batch:78 Train Loss:0.31384 Val Loss:0.31383\n",
      "Epoch:2/10 Batch:79 Train Loss:0.31383 Val Loss:0.31383\n",
      "Epoch:2/10 Batch:80 Train Loss:0.31383 Val Loss:0.31382\n",
      "Epoch:2/10 Batch:81 Train Loss:0.31382 Val Loss:0.31382\n",
      "Epoch:2/10 Batch:82 Train Loss:0.31382 Val Loss:0.31381\n",
      "Epoch:2/10 Batch:83 Train Loss:0.31381 Val Loss:0.31381\n",
      "Epoch:2/10 Batch:84 Train Loss:0.31381 Val Loss:0.31380\n",
      "Epoch:2/10 Batch:85 Train Loss:0.31380 Val Loss:0.31380\n",
      "Epoch:2/10 Batch:86 Train Loss:0.31380 Val Loss:0.31379\n",
      "Epoch:2/10 Batch:87 Train Loss:0.31379 Val Loss:0.31379\n",
      "Epoch:2/10 Batch:88 Train Loss:0.31379 Val Loss:0.31378\n",
      "Epoch:3/10 Batch:89 Train Loss:0.31378 Val Loss:0.31378\n",
      "Epoch:3/10 Batch:90 Train Loss:0.31378 Val Loss:0.31377\n",
      "Epoch:3/10 Batch:91 Train Loss:0.31377 Val Loss:0.31377\n",
      "Epoch:3/10 Batch:92 Train Loss:0.31377 Val Loss:0.31377\n",
      "Epoch:3/10 Batch:93 Train Loss:0.31377 Val Loss:0.31376\n",
      "Epoch:3/10 Batch:94 Train Loss:0.31376 Val Loss:0.31376\n",
      "Epoch:3/10 Batch:95 Train Loss:0.31376 Val Loss:0.31375\n",
      "Epoch:3/10 Batch:96 Train Loss:0.31375 Val Loss:0.31375\n",
      "Epoch:3/10 Batch:97 Train Loss:0.31375 Val Loss:0.31375\n",
      "Epoch:3/10 Batch:98 Train Loss:0.31375 Val Loss:0.31374\n",
      "Epoch:3/10 Batch:99 Train Loss:0.31374 Val Loss:0.31374\n",
      "Epoch:3/10 Batch:100 Train Loss:0.31374 Val Loss:0.31373\n",
      "Epoch:3/10 Batch:101 Train Loss:0.31373 Val Loss:0.31373\n",
      "Epoch:3/10 Batch:102 Train Loss:0.31373 Val Loss:0.31373\n",
      "Epoch:3/10 Batch:103 Train Loss:0.31373 Val Loss:0.31372\n",
      "Epoch:3/10 Batch:104 Train Loss:0.31372 Val Loss:0.31372\n",
      "Epoch:3/10 Batch:105 Train Loss:0.31372 Val Loss:0.31372\n",
      "Epoch:3/10 Batch:106 Train Loss:0.31372 Val Loss:0.31371\n",
      "Epoch:3/10 Batch:107 Train Loss:0.31371 Val Loss:0.31371\n",
      "Epoch:3/10 Batch:108 Train Loss:0.31371 Val Loss:0.31371\n",
      "Epoch:3/10 Batch:109 Train Loss:0.31371 Val Loss:0.31370\n",
      "Epoch:3/10 Batch:110 Train Loss:0.31370 Val Loss:0.31370\n",
      "Epoch:3/10 Batch:111 Train Loss:0.31370 Val Loss:0.31370\n",
      "Epoch:3/10 Batch:112 Train Loss:0.31370 Val Loss:0.31369\n",
      "Epoch:3/10 Batch:113 Train Loss:0.31369 Val Loss:0.31369\n",
      "Epoch:3/10 Batch:114 Train Loss:0.31369 Val Loss:0.31369\n",
      "Epoch:3/10 Batch:115 Train Loss:0.31369 Val Loss:0.31368\n",
      "Epoch:3/10 Batch:116 Train Loss:0.31368 Val Loss:0.31368\n",
      "Epoch:3/10 Batch:117 Train Loss:0.31368 Val Loss:0.31368\n",
      "Epoch:3/10 Batch:118 Train Loss:0.31368 Val Loss:0.31367\n",
      "Epoch:3/10 Batch:119 Train Loss:0.31367 Val Loss:0.31367\n",
      "Epoch:3/10 Batch:120 Train Loss:0.31367 Val Loss:0.31367\n",
      "Epoch:3/10 Batch:121 Train Loss:0.31367 Val Loss:0.31367\n",
      "Epoch:3/10 Batch:122 Train Loss:0.31367 Val Loss:0.31366\n",
      "Epoch:3/10 Batch:123 Train Loss:0.31366 Val Loss:0.31366\n",
      "Epoch:3/10 Batch:124 Train Loss:0.31366 Val Loss:0.31366\n",
      "Epoch:3/10 Batch:125 Train Loss:0.31366 Val Loss:0.31365\n",
      "Epoch:3/10 Batch:126 Train Loss:0.31365 Val Loss:0.31365\n",
      "Epoch:3/10 Batch:127 Train Loss:0.31365 Val Loss:0.31365\n",
      "Epoch:3/10 Batch:128 Train Loss:0.31365 Val Loss:0.31365\n",
      "Epoch:3/10 Batch:129 Train Loss:0.31365 Val Loss:0.31364\n",
      "Epoch:3/10 Batch:130 Train Loss:0.31364 Val Loss:0.31364\n",
      "Epoch:3/10 Batch:131 Train Loss:0.31364 Val Loss:0.31364\n",
      "Epoch:3/10 Batch:132 Train Loss:0.31364 Val Loss:0.31363\n",
      "Epoch:4/10 Batch:133 Train Loss:0.31363 Val Loss:0.31363\n",
      "Epoch:4/10 Batch:134 Train Loss:0.31363 Val Loss:0.31363\n",
      "Epoch:4/10 Batch:135 Train Loss:0.31363 Val Loss:0.31363\n",
      "Epoch:4/10 Batch:136 Train Loss:0.31363 Val Loss:0.31362\n",
      "Epoch:4/10 Batch:137 Train Loss:0.31362 Val Loss:0.31362\n",
      "Epoch:4/10 Batch:138 Train Loss:0.31362 Val Loss:0.31362\n",
      "Epoch:4/10 Batch:139 Train Loss:0.31362 Val Loss:0.31362\n",
      "Epoch:4/10 Batch:140 Train Loss:0.31362 Val Loss:0.31361\n",
      "Epoch:4/10 Batch:141 Train Loss:0.31361 Val Loss:0.31361\n",
      "Epoch:4/10 Batch:142 Train Loss:0.31361 Val Loss:0.31361\n",
      "Epoch:4/10 Batch:143 Train Loss:0.31361 Val Loss:0.31361\n",
      "Epoch:4/10 Batch:144 Train Loss:0.31361 Val Loss:0.31361\n",
      "Epoch:4/10 Batch:145 Train Loss:0.31361 Val Loss:0.31360\n",
      "Epoch:4/10 Batch:146 Train Loss:0.31360 Val Loss:0.31360\n",
      "Epoch:4/10 Batch:147 Train Loss:0.31360 Val Loss:0.31360\n",
      "Epoch:4/10 Batch:148 Train Loss:0.31360 Val Loss:0.31360\n",
      "Epoch:4/10 Batch:149 Train Loss:0.31360 Val Loss:0.31359\n",
      "Epoch:4/10 Batch:150 Train Loss:0.31359 Val Loss:0.31359\n",
      "Epoch:4/10 Batch:151 Train Loss:0.31359 Val Loss:0.31359\n",
      "Epoch:4/10 Batch:152 Train Loss:0.31359 Val Loss:0.31359\n",
      "Epoch:4/10 Batch:153 Train Loss:0.31359 Val Loss:0.31359\n",
      "Epoch:4/10 Batch:154 Train Loss:0.31359 Val Loss:0.31358\n",
      "Epoch:4/10 Batch:155 Train Loss:0.31358 Val Loss:0.31358\n",
      "Epoch:4/10 Batch:156 Train Loss:0.31358 Val Loss:0.31358\n",
      "Epoch:4/10 Batch:157 Train Loss:0.31358 Val Loss:0.31358\n",
      "Epoch:4/10 Batch:158 Train Loss:0.31358 Val Loss:0.31357\n",
      "Epoch:4/10 Batch:159 Train Loss:0.31358 Val Loss:0.31357\n",
      "Epoch:4/10 Batch:160 Train Loss:0.31357 Val Loss:0.31357\n",
      "Epoch:4/10 Batch:161 Train Loss:0.31357 Val Loss:0.31357\n",
      "Epoch:4/10 Batch:162 Train Loss:0.31357 Val Loss:0.31357\n",
      "Epoch:4/10 Batch:163 Train Loss:0.31357 Val Loss:0.31357\n",
      "Epoch:4/10 Batch:164 Train Loss:0.31357 Val Loss:0.31356\n",
      "Epoch:4/10 Batch:165 Train Loss:0.31356 Val Loss:0.31356\n",
      "Epoch:4/10 Batch:166 Train Loss:0.31356 Val Loss:0.31356\n",
      "Epoch:4/10 Batch:167 Train Loss:0.31356 Val Loss:0.31356\n",
      "Epoch:4/10 Batch:168 Train Loss:0.31356 Val Loss:0.31356\n",
      "Epoch:4/10 Batch:169 Train Loss:0.31356 Val Loss:0.31355\n",
      "Epoch:4/10 Batch:170 Train Loss:0.31355 Val Loss:0.31355\n",
      "Epoch:4/10 Batch:171 Train Loss:0.31355 Val Loss:0.31355\n",
      "Epoch:4/10 Batch:172 Train Loss:0.31355 Val Loss:0.31355\n",
      "Epoch:4/10 Batch:173 Train Loss:0.31355 Val Loss:0.31355\n",
      "Epoch:4/10 Batch:174 Train Loss:0.31355 Val Loss:0.31354\n",
      "Epoch:4/10 Batch:175 Train Loss:0.31354 Val Loss:0.31354\n",
      "Epoch:4/10 Batch:176 Train Loss:0.31354 Val Loss:0.31354\n",
      "Epoch:5/10 Batch:177 Train Loss:0.31354 Val Loss:0.31354\n",
      "Epoch:5/10 Batch:178 Train Loss:0.31354 Val Loss:0.31354\n",
      "Epoch:5/10 Batch:179 Train Loss:0.31354 Val Loss:0.31354\n",
      "Epoch:5/10 Batch:180 Train Loss:0.31354 Val Loss:0.31353\n",
      "Epoch:5/10 Batch:181 Train Loss:0.31353 Val Loss:0.31353\n",
      "Epoch:5/10 Batch:182 Train Loss:0.31353 Val Loss:0.31353\n",
      "Epoch:5/10 Batch:183 Train Loss:0.31353 Val Loss:0.31353\n",
      "Epoch:5/10 Batch:184 Train Loss:0.31353 Val Loss:0.31353\n",
      "Epoch:5/10 Batch:185 Train Loss:0.31353 Val Loss:0.31353\n",
      "Epoch:5/10 Batch:186 Train Loss:0.31353 Val Loss:0.31352\n",
      "Epoch:5/10 Batch:187 Train Loss:0.31352 Val Loss:0.31352\n",
      "Epoch:5/10 Batch:188 Train Loss:0.31352 Val Loss:0.31352\n",
      "Epoch:5/10 Batch:189 Train Loss:0.31352 Val Loss:0.31352\n",
      "Epoch:5/10 Batch:190 Train Loss:0.31352 Val Loss:0.31352\n",
      "Epoch:5/10 Batch:191 Train Loss:0.31352 Val Loss:0.31352\n",
      "Epoch:5/10 Batch:192 Train Loss:0.31352 Val Loss:0.31352\n",
      "Epoch:5/10 Batch:193 Train Loss:0.31352 Val Loss:0.31351\n",
      "Epoch:5/10 Batch:194 Train Loss:0.31351 Val Loss:0.31351\n",
      "Epoch:5/10 Batch:195 Train Loss:0.31351 Val Loss:0.31351\n",
      "Epoch:5/10 Batch:196 Train Loss:0.31351 Val Loss:0.31351\n",
      "Epoch:5/10 Batch:197 Train Loss:0.31351 Val Loss:0.31351\n",
      "Epoch:5/10 Batch:198 Train Loss:0.31351 Val Loss:0.31351\n",
      "Epoch:5/10 Batch:199 Train Loss:0.31351 Val Loss:0.31351\n",
      "Epoch:5/10 Batch:200 Train Loss:0.31351 Val Loss:0.31350\n",
      "Epoch:5/10 Batch:201 Train Loss:0.31350 Val Loss:0.31350\n",
      "Epoch:5/10 Batch:202 Train Loss:0.31350 Val Loss:0.31350\n",
      "Epoch:5/10 Batch:203 Train Loss:0.31350 Val Loss:0.31350\n",
      "Epoch:5/10 Batch:204 Train Loss:0.31350 Val Loss:0.31350\n",
      "Epoch:5/10 Batch:205 Train Loss:0.31350 Val Loss:0.31350\n",
      "Epoch:5/10 Batch:206 Train Loss:0.31350 Val Loss:0.31350\n",
      "Epoch:5/10 Batch:207 Train Loss:0.31350 Val Loss:0.31349\n",
      "Epoch:5/10 Batch:208 Train Loss:0.31349 Val Loss:0.31349\n",
      "Epoch:5/10 Batch:209 Train Loss:0.31349 Val Loss:0.31349\n",
      "Epoch:5/10 Batch:210 Train Loss:0.31349 Val Loss:0.31349\n",
      "Epoch:5/10 Batch:211 Train Loss:0.31349 Val Loss:0.31349\n",
      "Epoch:5/10 Batch:212 Train Loss:0.31349 Val Loss:0.31349\n",
      "Epoch:5/10 Batch:213 Train Loss:0.31349 Val Loss:0.31349\n",
      "Epoch:5/10 Batch:214 Train Loss:0.31349 Val Loss:0.31349\n",
      "Epoch:5/10 Batch:215 Train Loss:0.31349 Val Loss:0.31348\n",
      "Epoch:5/10 Batch:216 Train Loss:0.31348 Val Loss:0.31348\n",
      "Epoch:5/10 Batch:217 Train Loss:0.31348 Val Loss:0.31348\n",
      "Epoch:5/10 Batch:218 Train Loss:0.31348 Val Loss:0.31348\n",
      "Epoch:5/10 Batch:219 Train Loss:0.31348 Val Loss:0.31348\n",
      "Epoch:5/10 Batch:220 Train Loss:0.31348 Val Loss:0.31348\n",
      "Epoch:6/10 Batch:221 Train Loss:0.31348 Val Loss:0.31348\n",
      "Epoch:6/10 Batch:222 Train Loss:0.31348 Val Loss:0.31348\n",
      "Epoch:6/10 Batch:223 Train Loss:0.31348 Val Loss:0.31348\n",
      "Epoch:6/10 Batch:224 Train Loss:0.31347 Val Loss:0.31347\n",
      "Epoch:6/10 Batch:225 Train Loss:0.31347 Val Loss:0.31347\n",
      "Epoch:6/10 Batch:226 Train Loss:0.31347 Val Loss:0.31347\n",
      "Epoch:6/10 Batch:227 Train Loss:0.31347 Val Loss:0.31347\n",
      "Epoch:6/10 Batch:228 Train Loss:0.31347 Val Loss:0.31347\n",
      "Epoch:6/10 Batch:229 Train Loss:0.31347 Val Loss:0.31347\n",
      "Epoch:6/10 Batch:230 Train Loss:0.31347 Val Loss:0.31347\n",
      "Epoch:6/10 Batch:231 Train Loss:0.31347 Val Loss:0.31347\n",
      "Epoch:6/10 Batch:232 Train Loss:0.31347 Val Loss:0.31347\n",
      "Epoch:6/10 Batch:233 Train Loss:0.31347 Val Loss:0.31346\n",
      "Epoch:6/10 Batch:234 Train Loss:0.31346 Val Loss:0.31346\n",
      "Epoch:6/10 Batch:235 Train Loss:0.31346 Val Loss:0.31346\n",
      "Epoch:6/10 Batch:236 Train Loss:0.31346 Val Loss:0.31346\n",
      "Epoch:6/10 Batch:237 Train Loss:0.31346 Val Loss:0.31346\n",
      "Epoch:6/10 Batch:238 Train Loss:0.31346 Val Loss:0.31346\n",
      "Epoch:6/10 Batch:239 Train Loss:0.31346 Val Loss:0.31346\n",
      "Epoch:6/10 Batch:240 Train Loss:0.31346 Val Loss:0.31346\n",
      "Epoch:6/10 Batch:241 Train Loss:0.31346 Val Loss:0.31346\n",
      "Epoch:6/10 Batch:242 Train Loss:0.31346 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:243 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:244 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:245 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:246 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:247 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:248 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:249 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:250 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:251 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:252 Train Loss:0.31345 Val Loss:0.31345\n",
      "Epoch:6/10 Batch:253 Train Loss:0.31345 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:254 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:255 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:256 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:257 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:258 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:259 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:260 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:261 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:262 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:263 Train Loss:0.31344 Val Loss:0.31344\n",
      "Epoch:6/10 Batch:264 Train Loss:0.31344 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:265 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:266 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:267 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:268 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:269 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:270 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:271 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:272 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:273 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:274 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:275 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:276 Train Loss:0.31343 Val Loss:0.31343\n",
      "Epoch:7/10 Batch:277 Train Loss:0.31343 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:278 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:279 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:280 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:281 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:282 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:283 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:284 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:285 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:286 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:287 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:288 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:289 Train Loss:0.31342 Val Loss:0.31342\n",
      "Epoch:7/10 Batch:290 Train Loss:0.31342 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:291 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:292 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:293 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:294 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:295 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:296 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:297 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:298 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:299 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:300 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:301 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:302 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:303 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:304 Train Loss:0.31341 Val Loss:0.31341\n",
      "Epoch:7/10 Batch:305 Train Loss:0.31341 Val Loss:0.31340\n",
      "Epoch:7/10 Batch:306 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:7/10 Batch:307 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:7/10 Batch:308 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:309 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:310 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:311 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:312 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:313 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:314 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:315 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:316 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:317 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:318 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:319 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:320 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:321 Train Loss:0.31340 Val Loss:0.31340\n",
      "Epoch:8/10 Batch:322 Train Loss:0.31340 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:323 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:324 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:325 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:326 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:327 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:328 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:329 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:330 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:331 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:332 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:333 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:334 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:335 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:336 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:337 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:338 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:339 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:340 Train Loss:0.31339 Val Loss:0.31339\n",
      "Epoch:8/10 Batch:341 Train Loss:0.31339 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:342 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:343 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:344 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:345 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:346 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:347 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:348 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:349 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:350 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:351 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:8/10 Batch:352 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:353 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:354 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:355 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:356 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:357 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:358 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:359 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:360 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:361 Train Loss:0.31338 Val Loss:0.31338\n",
      "Epoch:9/10 Batch:362 Train Loss:0.31338 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:363 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:364 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:365 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:366 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:367 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:368 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:369 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:370 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:371 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:372 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:373 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:374 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:375 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:376 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:377 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:378 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:379 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:380 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:381 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:382 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:383 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:384 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:385 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:386 Train Loss:0.31337 Val Loss:0.31337\n",
      "Epoch:9/10 Batch:387 Train Loss:0.31337 Val Loss:0.31336\n",
      "Epoch:9/10 Batch:388 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:9/10 Batch:389 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:9/10 Batch:390 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:9/10 Batch:391 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:9/10 Batch:392 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:9/10 Batch:393 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:9/10 Batch:394 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:9/10 Batch:395 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:9/10 Batch:396 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:397 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:398 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:399 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:400 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:401 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:402 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:403 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:404 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:405 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:406 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:407 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:408 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:409 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:410 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:411 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:412 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:413 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:414 Train Loss:0.31336 Val Loss:0.31336\n",
      "Epoch:10/10 Batch:415 Train Loss:0.31336 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:416 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:417 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:418 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:419 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:420 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:421 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:422 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:423 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:424 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:425 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:426 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:427 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:428 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:429 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:430 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:431 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:432 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:433 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:434 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:435 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:436 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:437 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:438 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:439 Train Loss:0.31335 Val Loss:0.31335\n",
      "Epoch:10/10 Batch:440 Train Loss:0.31335 Val Loss:0.31335\n"
     ]
    }
   ],
   "source": [
    "# defining hyperparameters\n",
    "\n",
    "n_epochs = 10\n",
    "lr = 1e-4\n",
    "counter = 0\n",
    "clip = 5\n",
    "\n",
    "# defining loss and optimzier functions\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimzier = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "  #initialize hidden state\n",
    "  h = model.init_hidden(batch_size)\n",
    "\n",
    "  #batch loop\n",
    "  for inputs, labels in train_loader:\n",
    "    counter+=1\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    model.zero_grad()\n",
    "    outputs,h = model(inputs,h)\n",
    "    loss = criterion(outputs,torch.max(labels,1)[1])\n",
    "    loss.backward()\n",
    "\n",
    "    #using clip grad norm which prevents exploding gradient prob in RNN\n",
    "    nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "    optimzier.step()\n",
    "\n",
    "    #validation loss\n",
    "    \n",
    "    val_h = model.init_hidden(batch_size).cuda()\n",
    "    val_losses =[]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for inputs,labels in val_loader:\n",
    "  \n",
    "      inputs,labels = inputs.to(device), labels.to(device)\n",
    "      val_outputs,val_h = model(inputs,val_h)\n",
    "      val_loss = criterion(val_outputs,torch.max(labels,1)[1])\n",
    "      val_losses.append(val_loss.item())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('Epoch:{}/{}'.format(epoch+1,n_epochs),\n",
    "          'Batch:{}'.format(counter),\n",
    "          'Train Loss:{:.5f}'.format(loss.item()),\n",
    "          'Val Loss:{:.5f}'.format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVnZxFxT49cl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chinmay_Gupta_3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
